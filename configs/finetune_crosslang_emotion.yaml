# =============================================================================
# F5-TTS Enhanced: Cross-Language + Emotion Fine-tuning Config
# =============================================================================
#
# Двухэтапное обучение:
#   Этап 1: Emotion Extractor (на ESD + RAVDESS)
#   Этап 2: F5-TTS + PEFT адаптеры + Emotion Conditioning
#
# Предполагается файнтюнинг от F5TTS_v1_Base чекпоинта.
# =============================================================================

# --- Базовая модель ---
base_model:
  name: F5TTS_v1_Base
  checkpoint: "hf://ESpeech/ESpeech-TTS-1_RL-V2/resolve/main/espeech_tts_rlv2.pt"
  vocab: "hf://ESpeech/ESpeech-TTS-1_RL-V2/resolve/main/vocab.txt"
  # Если нужен multilingual vocab — укажите свой путь:
  # vocab: "./vocab_multilingual.txt"

# --- Архитектура модели ---
model:
  arch:
    dim: 1024           # hidden dimension DiT
    depth: 22            # число DiT блоков
    heads: 16            # attention heads
    ff_mult: 2           # FFN multiplier
    text_dim: 512        # text embedding dimension
    conv_layers: 4       # ConvNeXt V2 блоков

  mel_spec:
    target_sample_rate: 24000
    n_mel_channels: 100
    hop_length: 256
    win_length: 1024
    n_fft: 1024
    mel_spec_type: vocos   # 'vocos' или 'bigvgan'

  vocoder:
    is_local: false
    local_path: null

# --- Emotion Extractor (предобученная emotion2vec — обучение НЕ нужно) ---
emotion:
  extractor:
    # Используем предобученную emotion2vec+ (ACL 2024)
    # Обучена на 42K+ часов эмоциональных данных, 10+ языков
    model_size: large          # "large" (300M, лучшее) | "base" (90M) | "seed" (90M)
    emotion_dim: 768           # нативная размерность emotion2vec
    hub: hf                    # "hf" (HuggingFace) | "ms" (ModelScope для Китая)
    cache_dir: null            # null = ~/.cache/f5tts_emotion_cache
    # pip install funasr       — единственная зависимость

  conditioning:
    mode: adaln              # 'adaln' или 'cross_attention'
    # adaln — быстрее, стабильнее, рекомендуется для начала
    # cross_attention — более выразительный, но дольше обучается

  # !! ОБУЧЕНИЕ ЭТАПА 1 БОЛЬШЕ НЕ НУЖНО !!
  # emotion2vec уже обучена на: ESD, RAVDESS, CREMA-D, IEMOCAP,
  # и ещё десятках датасетов (всего 42K часов)
  # Просто pip install funasr и используйте

# --- PEFT Адаптеры ---
adapters:
  freeze_base: true          # заморозить оригинальную F5-TTS

  # LoRA для DiT Q,V проекций
  lora:
    rank: 16                 # rank=16 оптимален для баланса
    alpha: 16.0              # scaling = alpha / rank = 1.0
    dropout: 0.05
    target_modules:
      - to_q
      - to_v

  # Conditioning Adapter для ConvNeXt
  conditioning_adapter:
    compression_factor: 0.25  # ~50K params per adapter
    kernel_size: 3

  # Prompt Adapter
  prompt_adapter:
    rank: 16
    drop_path_rate: 0.3      # DropPath для регуляризации

  # Language Embedding
  language_embedding:
    enabled: true
    num_languages: 2

# --- Данные для файнтюнинга ---
datasets:
  # Путь к подготовленным данным (выход prepare_multilingual_dataset.py)
  data_root: "/data/prepared"                # корневая директория
  metadata_train: "/data/prepared/metadata.csv"
  metadata_val: "/data/prepared/metadata_val.csv"

  # Или список исходных директорий (для prepare_multilingual_dataset.py):
  multilingual:
    - name: emilia_en
      path: "/data/emilia/en"
      language: en
      weight: 1.0
    - name: your_target_language     # ваш целевой язык
      path: "/data/your_lang"
      language: ru                    # замените на свой
      weight: 2.0                     # повышенный вес для целевого языка
  
  # Формат metadata.csv (pipe-separated):
  #   audio_path|text|language|duration|emotion
  #   wavs/en_000001.wav|Hello world|en|2.5|happy
  #   wavs/ru_000001.wav|Привет мир|ru|2.3|
  
  batch_size_per_gpu: 3200    # в Mel-фреймах (динамический батчинг)
  batch_size_type: frame
  max_samples: 64             # макс сэмплов в батче
  num_workers: 4
  max_duration: 30.0          # секунд

# --- Оптимизация ---
optim:
  epochs: 100                  # или steps (см. ниже)
  max_steps: 150000            # ~150K steps для файнтюнинга
  learning_rate: 1.0e-5        # Ниже чем при обучении с нуля
  weight_decay: 0.01
  
  # Warm-up
  num_warmup_updates: 2000
  
  # Gradient
  grad_accumulation_steps: 4   # effective batch = 4 × 3200 = 12800 frames
  max_grad_norm: 1.0
  
  # Optimizer
  optimizer: adamw             # или bnb_adamw для 8-bit
  bnb_optimizer: false         # true для экономии VRAM на RTX 4090
  
  # Scheduler
  scheduler: cosine_with_restarts
  
  # Mixed precision
  mixed_precision: bf16        # bf16 рекомендуется для стабильности

# --- Логирование и чекпоинты ---
logging:
  logger: tensorboard          # 'tensorboard' или 'wandb'
  log_dir: "logs/crosslang_emotion"
  log_samples: true            # генерировать sample аудио при логировании
  log_every_steps: 100
  
checkpoint:
  save_dir: "ckpts/enhanced"
  save_per_updates: 5000       # каждые 5K шагов
  last_per_steps: 1000         # last checkpoint каждые 1K
  keep_last_n: 3               # хранить последние 3 чекпоинта
  
  # Для resume
  resume_from: null             # путь к чекпоинту для продолжения

# --- Evaluation ---
evaluation:
  eval_every_steps: 5000
  
  # Reference аудио для генерации samples
  eval_refs:
    #- audio: "eval/ref_en_neutral.wav"
     # text: "Some call me nature, others call me mother nature."
      #gen_text: "I don't care what you call me, I've been watching species evolve."
      #gen_lang: en
    #- audio: "eval/ref_en_happy.wav"
     # text: "This is absolutely wonderful news!"
      #gen_text: "Это замечательная новость, я очень рада!"
      #gen_lang: ru
   # - audio: "eval/ref_zh.wav"
    #  text: "对，这就是我，万人敬仰的太乙真人。"
     # gen_text: "Yes, that's me, the greatly admired immortal."
     # gen_lang: en
  
  metrics:
    - WER          # Word Error Rate (Whisper transcription)
    - SIM-O        # Speaker Similarity (WavLM embeddings)
    - UTMOS        # Predicted MOS score
    - F0_CORR      # F0 correlation (для оценки просодии)
